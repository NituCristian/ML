Daca learning rate-ul este foarte mic, reteaua noastra va "invata" mai greu 
=> functia de loss va scadea mult mai incet (dupa mult mai multe epoci)
Acest lucru are loc din cauza ca backward propagation va realiza un update mult mai mic pentru weight-uri

In schimb, daca marim learning rate-ul, functia de loss poate diverge. Astfel, ea va avea, destul de rapid,
valori de ordinul miilor, si va putea creste, ajungand dupa un numar de epoci, pana la Infinit (sau chiar NAN).

Putem vedea ca, daca crestem foarte mult latimea retelei (ex: 1000 de neuroni in hidden layer), atunci functia 
de loss poate avea, de asemenea, valori mari, fara modificarea learning rate-ului
(in general se afla intre 700 si 1400 daca avem un batch size = 32), ceea ce ne evidentiaza faptul ca ar trebui
ca in hidden layers sa avem un numar de neuroni ceva mai mic

Daca adaugam mai multe nivele de adancime, observam o valoare destul de scazuta a functiei de loss,
aceasta modificare (eventual impreuna cu cresterea latimii retelei), avand posibilitatea de a duce la 
overfitting, intrucat reteaua neuronala va invata mult mai bine toate exemplele din dataset, iar, in cazul in care
testam pe un alt exemplu (diferit fata de dataset), s-ar putea ca aceasta sa nu mai prezica bine output-ul

Daca crestem batch size-ul, observam ca functia de loss isi va modifica si ea valoarea (size 64 vs 32 
=> functia de loss isi dubleaza valoarea)